{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Convolutional Layers in a Trained VGG Network\n",
    "\n",
    "### What have the various feature maps in a CNN been trianed to look for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "---\n",
    "\n",
    "Convolutional Neural Nets (CNNs) are very deep data sructutes that extract increasingly complex spacial patterns in 2d or 3d data.\n",
    "\n",
    "A CNN trained on a large corpus of images will build representations of the training at different levels of complexity at different layers of the network. In theory, lower layers will build representations of low level features such as colors and edges. Then higer levels will combine thos representations into increasingly complex feature detectors, e.g. edges -> corners -> squares -> doors -> houses\n",
    "\n",
    "This ability for CNNs to find patterns within patterns within patterns makes them extrmely powerful, but also extremly mysterious. Is there a way to increase the interpretability of what CNNs are doing as an image is fed forward through the network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods\n",
    "\n",
    "---\n",
    "\n",
    "In this exploration, I will be using a pretrianed VGG16 network wo build an image that most intensly 'excites' a subset of feature maps in specific convolutional layers.\n",
    "\n",
    "The convolutional layers I will be examining are:\n",
    "- Conv1_1 with 64 feature maps to explore\n",
    "- Conv2_1 with 128 feature maps to explore\n",
    "- Conv3_1 with 256 feature maps to explore\n",
    "- Conv4_1 with 512 feature maps to explore\n",
    "- Conv5_1 with 512 feature maps to explore\n",
    "\n",
    "<img src=\"./assets/readme_a.png\" width=\"300\"/>\n",
    "\n",
    "I do this by starting with a targer tensor of random noise and feeding it through the pretrained VGG network which has had it's parameters frozen. At a specific feature map in the layer I am examining, I grab the activations calculate a loss by comparing the actual activation tensor to a target activation that is unreachably high (In this case a target tensor filled with the maximum squared value in the actual tensor). I then backpropogate the loss to update the target tensor; In this way after each iteration it continually updates into an input that maximally actucated the feature map in question.\n",
    "\n",
    "I then display the tensor as an image for an idea of what that particualr feature map has been trianed to look for! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7fd5427e9350>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "vgg = torchvision.models.vgg16(pretrained=True).to(device)\n",
    "\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "vgg.activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        vgg.activation[name] = output.squeeze()\n",
    "    return hook\n",
    "# mapping between layers indexed in model and layer names\n",
    "layers = {'conv1_1': 0,\n",
    "          'conv2_1': 5, \n",
    "          'conv3_1': 10, \n",
    "          'conv4_1': 17,\n",
    "          'conv5_1': 24,}\n",
    "\n",
    "vgg.features[layers['conv1_1']].register_forward_hook(get_activation('Conv1_1'))\n",
    "vgg.features[layers['conv2_1']].register_forward_hook(get_activation('Conv2_1'))\n",
    "vgg.features[layers['conv3_1']].register_forward_hook(get_activation('Conv3_1'))\n",
    "vgg.features[layers['conv4_1']].register_forward_hook(get_activation('Conv4_1'))\n",
    "vgg.features[layers['conv5_1']].register_forward_hook(get_activation('Conv5_1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(img_path):\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    in_transform = transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                                             (0.229, 0.224, 0.225))])\n",
    "    image = in_transform(image).unsqueeze(0)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im_convert(tensor):\n",
    "    \"\"\" Display a tensor as an image. \"\"\"\n",
    "    \n",
    "    image = tensor.to(\"cpu\").clone().detach()\n",
    "    image = image.numpy().squeeze()\n",
    "    image = image.transpose(1,2,0)\n",
    "    image = image * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))\n",
    "    image = image.clip(0, 1)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(layer_to_visualize='Conv1_1',\n",
    "        filers_to_show=(2,2),\n",
    "        resolution=100,\n",
    "        steps=200, lr=0.01,\n",
    "        shift=0, dist=(-1., 1.),\n",
    "        random_state=None):\n",
    "\n",
    "  random = np.random.RandomState(random_state) if random_state is not None else np.random\n",
    "  fig, axs = plt.subplots(filers_to_show[0],\n",
    "                          filers_to_show[1],\n",
    "                          figsize=(filers_to_show[1]*2,filers_to_show[0]*2+0.3),\n",
    "                          constrained_layout=True)\n",
    "  if hasattr(axs, '__len__') == False:\n",
    "    axs = np.array([axs])\n",
    "  axs = axs.reshape(filers_to_show[0], filers_to_show[1])\n",
    "\n",
    "  for x in range(filers_to_show[0]):\n",
    "    for y in range(filers_to_show[1]):\n",
    "      map_number = (x * (filers_to_show[1])) + y + shift\n",
    "      target_image = torch.from_numpy(random.uniform(*dist,\n",
    "                                      size=(3,resolution,resolution))).unsqueeze(0)\n",
    "      target = target_image.clone().type(torch.FloatTensor).to(device).requires_grad_(True)\n",
    "      optimizer = optim.Adam([target], lr=lr)\n",
    "\n",
    "      vgg.eval()\n",
    "\n",
    "      for ii in range(1, steps+1):\n",
    "        vgg.forward(target)\n",
    "        output = vgg.activation[layer_to_visualize][map_number]\n",
    "\n",
    "        out = output.detach()\n",
    "        expected = np.empty((*list(out.size()),))\n",
    "        expected.fill(float(torch.max(out)))\n",
    "        expected = expected**2\n",
    "        expected = torch.from_numpy(expected).to(device).requires_grad_(False)\n",
    "\n",
    "        loss = torch.mean((output - expected)**2)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "      axs[x][y].imshow(im_convert(target))\n",
    "      axs[x][y].axis('off')\n",
    "      axs[x][y].annotate(map_number,\n",
    "            xy=(0, 0), color='white',\n",
    "            fontweight='bold',\n",
    "            verticalalignment='top',\n",
    "            bbox=dict(boxstyle=\"round\", fc=\"black\"))\n",
    "  \n",
    "  fig.suptitle(layer_to_visualize, color='white', fontweight='bold')\n",
    "  fig.patch.set_facecolor('black')\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show('Conv1_1', (2,5), resolution=100, shift=20, random_state=100, dist=(0., 2.))\n",
    "show('Conv2_1', (2,5), resolution=110, shift=20, random_state=100, dist=(0., 0.5))\n",
    "show('Conv3_1', (2,5), resolution=120, shift=20, random_state=100, dist=(-0.1, 0.2))\n",
    "show('Conv4_1', (2,5), resolution=140, shift=20, random_state=100, dist=(-0.2, 0.2))\n",
    "show('Conv5_1', (2,5), resolution=160, shift=25, random_state=100, dist=(-0.25, 0.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "---\n",
    "\n",
    "Overall the results are quite impressive. You can obseve that in lower levels such as Conv1_1 feature maps are activated by relitivly solid colors and simple lines and shapes. I can deduce that this layer's feature maps have, over the course of training on the images dataset, learned to become simple color and edge detectors.\n",
    "\n",
    "This is in contrast to the highest layers which are activated by complicated and abstract patterns. These generated images are neat to look at, but some have enough form to actually guess what high level feature they are detecting!\n",
    "For exapmle, I found the 25th feature map of the Conv5_1 layer to be excited specifically by patterns that resemble human eyes! do you see any eyes staring back at you grom this generated image?\n",
    "\n",
    "<img src=\"./assets/output1.png\" width=\"200\"/>\n",
    "\n",
    "I can also see what resemble animal eyes in the 116th feature map of the Conv5_1 layer and what strike me as perhaps birds/parakeets in the 104th feature map of the Conv5_1 layer\n",
    "\n",
    "<img src=\"./assets/output2.jpeg\" width=\"400\"/>\n",
    "\n",
    "You can also observe blank and noisy squares generated for some feature maps, which appear to be more frequent in higher layers.\n",
    "At first I thought that it might mean that feature map did not actually learn anything useful during training, but i think the reality is that this method of backpropogation used to sensitive to starting conditions. I suspect that higher layers have a harder time finding the complex pattern that excites thim in the noisy starting tensor and never converge to anything meaningful. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0412b99d707ea6cfd7efb8d04aee40f3a7d54b9d99a7d3ef10aac525cbc76e55"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('kitchen_sink')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
