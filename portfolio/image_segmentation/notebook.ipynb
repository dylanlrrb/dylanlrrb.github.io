{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from pycocotools.coco import COCO\n",
    "import os\n",
    "\n",
    "os.chdir('/root')\n",
    "dir = '../tf/notebooks/portfolio/image_segmentation'\n",
    "\n",
    "train_annotations = COCO('datasets/coco2017/annotations/instances_train2017.json')\n",
    "val_annotations = COCO('datasets/coco2017/annotations/instances_val2017.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catIDs = train_annotations.getCatIds()\n",
    "cats = train_annotations.loadCats(catIDs)\n",
    "\n",
    "print(f\"Number of Unique Categories: {len(catIDs)}\")\n",
    "print(\"Categories Names:\")\n",
    "cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filterClasses = ['laptop', 'tv', 'cat']\n",
    "\n",
    "# Fetch class IDs only corresponding to the filterClasses\n",
    "catIds = train_annotations.getCatIds(catNms=filterClasses) \n",
    "# Get all images containing the above Category IDs\n",
    "imgIds = train_annotations.getImgIds(catIds=catIds)\n",
    "print(train_annotations.getCatIds())\n",
    "print(\"Number of images containing all the  classes:\", len(imgIds))\n",
    "print('imgIds:', imgIds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "%matplotlib inline\n",
    "\n",
    "imgId = imgIds[10]\n",
    "print('Image ID:', imgId)\n",
    "\n",
    "img_meta = train_annotations.loadImgs(imgId)[0]\n",
    "img = Image.open(f'datasets/coco2017/images/train2017/{img_meta[\"file_name\"]}')\n",
    "\n",
    "ann_ids = train_annotations.getAnnIds(imgIds=[imgId], iscrowd=None)\n",
    "anns = train_annotations.loadAnns(ann_ids)\n",
    "\n",
    "print('Example annotation for one object in scene:')\n",
    "print(anns[0])\n",
    "\n",
    "# Show image\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "# Show bounding boxes and segmentation\n",
    "plt.imshow(img)\n",
    "train_annotations.showAnns(anns, draw_bbox=True)\n",
    "plt.show()\n",
    "\n",
    "# Show masks\n",
    "mask = np.zeros((img_meta['height'],img_meta['width']))\n",
    "for i in range(len(anns)):\n",
    "    pixel_value = i+1\n",
    "    mask = np.maximum(train_annotations.annToMask(anns[i])*pixel_value, mask)\n",
    "plt.imshow(mask)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = [len(train_annotations.getImgIds(catIds=cat['id'])) for cat in cats]\n",
    "class_names = [cat['name'] for cat in cats]\n",
    "figure(figsize=(16,8), dpi=80)\n",
    "plt.bar(class_names, class_counts)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import gc\n",
    "\n",
    "# Create a data generator to generate batches of image/segmentation pairs\n",
    "# The ground truth will be a rank-3 tensor with 80 channels, where each channel represents a binary mask of a single class\n",
    "\n",
    "class CategoryMappingHelper():\n",
    "  def __init__(self, coco_annotations):\n",
    "    self.catIds = coco_annotations.getCatIds()\n",
    "    self.categories = [*coco_annotations.loadCats(self.catIds), {'id': 0, 'name': 'background'}]\n",
    "    self.filter_idx_to_category_id = [cat['id'] for cat in self.categories]\n",
    "    self.filter_idx_to_category_name = [cat['name'] for cat in self.categories]\n",
    "    self.categry_id_to_category_name = {cat['id']: cat['name'] for cat in self.categories}\n",
    "    self.category_id_to_filter_idx = {id: idx for idx, id in enumerate(self.filter_idx_to_category_id)}\n",
    "    self.category_name_to_filter_idx = {name: idx for idx, name in enumerate(self.filter_idx_to_category_name)}\n",
    "  def get (self, value, in_key, out_key):\n",
    "    if   (in_key == 'filter_idx' and out_key == 'category_id'):\n",
    "      return self.filter_idx_to_category_id[value]\n",
    "    elif (in_key == 'filter_idx' and out_key == 'category_name'):\n",
    "      return self.filter_idx_to_category_name[value]\n",
    "    elif (in_key == 'category_id' and out_key == 'category_name'):\n",
    "      return self.categry_id_to_category_name[value]\n",
    "    elif (in_key == 'category_id' and out_key == 'filter_idx'):\n",
    "      return self.category_id_to_filter_idx[value]\n",
    "    elif (in_key == 'category_name' and out_key == 'filter_idx'):\n",
    "      return self.category_name_to_filter_idx[value]\n",
    "    else:\n",
    "      raise Exception(\"in_key:out_key pair not supported. Valid pairs: \\nfilter_idx:category_id\\nfilter_idx:category_name\\ncategory_id:category_name\\ncategory_id:filter_idx\\ncategory_name:filter_idx\")\n",
    "\n",
    "\n",
    "def cocoDataGenerator(coco_annotations, image_folder, catMapper, batch_size, shuffle=False, input_size=(128,128), output_size=(128,128)):\n",
    "  numCategories = len(catMapper.catIds)\n",
    "  imageIds = coco_annotations.getImgIds()\n",
    "  if shuffle:\n",
    "    np.random.shuffle(imageIds)\n",
    "\n",
    "  X_batch = []\n",
    "  y_batch = []\n",
    "  \n",
    "  for imageId in imageIds:\n",
    "    img_meta = coco_annotations.loadImgs(imageId)[0]\n",
    "    ann_ids = coco_annotations.getAnnIds(imgIds=[imageId], iscrowd=None)\n",
    "    anns = coco_annotations.loadAnns(ann_ids)\n",
    "\n",
    "    X = Image.open(f'{image_folder}/{img_meta[\"file_name\"]}')\n",
    "    X = np.array(X)\n",
    "    X = cv2.resize(X, input_size)\n",
    "\n",
    "    masks = [np.zeros(output_size)] * (numCategories)\n",
    "    bg = np.zeros(output_size)\n",
    "    for ann in anns:\n",
    "      # if catMapper.get(ann['category_id'], 'category_id', 'category_name') != 'person':\n",
    "      new_mask = cv2.resize(coco_annotations.annToMask(ann), output_size)\n",
    "      # create mask for the background\n",
    "      bg = np.maximum(new_mask, bg)\n",
    "      prev_mask = masks[catMapper.get(ann['category_id'], 'category_id', 'filter_idx')]\n",
    "      combined_masks = np.maximum(new_mask, prev_mask)\n",
    "      masks[catMapper.get(ann['category_id'], 'category_id', 'filter_idx')] = combined_masks\n",
    "    bg =  np.logical_not(bg).astype(int)\n",
    "    masks.append(bg)\n",
    "    y = np.dstack(masks)\n",
    "    \n",
    "    if X.shape == (*input_size,3) and y.shape == (*output_size,numCategories+1):\n",
    "      X_batch.append(X)\n",
    "      y_batch.append(y)\n",
    "\n",
    "    if len(X_batch) == batch_size:\n",
    "      yield np.array(X_batch), np.array(y_batch)\n",
    "      X_batch = []\n",
    "      y_batch = []\n",
    "      gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test out generator\n",
    "catMapper = CategoryMappingHelper(train_annotations)\n",
    "test_generator = cocoDataGenerator(train_annotations, 'datasets/coco2017/images/train2017', catMapper, batch_size=1, shuffle=False)\n",
    "test_iterator = iter(test_generator)\n",
    "\n",
    "X_batch, y_batch = next(test_generator)\n",
    "X, y = (X_batch[0], y_batch[0])\n",
    "plt.imshow(X)\n",
    "plt.show()\n",
    "plt.title('motorcycle mask from ground truth')\n",
    "plt.imshow(y[:,:,catMapper.get('motorcycle', 'category_name', 'filter_idx')])\n",
    "plt.show()\n",
    "plt.title('person mask from ground truth')\n",
    "plt.imshow(y[:,:,catMapper.get('person', 'category_name', 'filter_idx')])\n",
    "plt.show()\n",
    "plt.title('background mask from ground truth')\n",
    "plt.imshow(y[:,:,-1])\n",
    "plt.show()\n",
    "\n",
    "X_batch, y_batch = next(test_generator)\n",
    "X, y = (X_batch[0], y_batch[0])\n",
    "plt.imshow(X)\n",
    "plt.show()\n",
    "plt.title('person mask from ground truth')\n",
    "plt.imshow(y[:,:,catMapper.get('person', 'category_name', 'filter_idx')])\n",
    "plt.show()\n",
    "plt.title('knife mask from ground truth')\n",
    "plt.imshow(y[:,:,catMapper.get('knife', 'category_name', 'filter_idx')])\n",
    "plt.show()\n",
    "plt.title('background mask from ground truth')\n",
    "plt.imshow(y[:,:,-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def augment(data_gen, imageDataGeneratorArgs={}, seed=None):\n",
    "  rng = np.random.default_rng(seed if seed is not None else np.random.choice(range(9999)))\n",
    "\n",
    "  X_gen = ImageDataGenerator(**imageDataGeneratorArgs)\n",
    "  imageDataGeneratorArgs_mask = imageDataGeneratorArgs.copy()\n",
    "   # Remove the brightness argument for the binary masks but keep spatial augmentations.\n",
    "  imageDataGeneratorArgs_mask.pop('brightness_range', None)\n",
    "  y_gen = ImageDataGenerator(**imageDataGeneratorArgs_mask)\n",
    "  \n",
    "  for X_batch, y_batch in data_gen:\n",
    "    seed = rng.choice(range(9999))\n",
    "    g_x = X_gen.flow(X_batch, \n",
    "                    batch_size = X_batch.shape[0], \n",
    "                    seed = seed, \n",
    "                    shuffle=False)\n",
    "    g_y = y_gen.flow(y_batch, \n",
    "                    batch_size = y_batch.shape[0], \n",
    "                    seed = seed, \n",
    "                    shuffle=False)\n",
    "      \n",
    "    X_aug = next(g_x)\n",
    "    y_aug = next(g_y)\n",
    "                \n",
    "    yield X_aug, y_aug\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show that augmentation works on test_generator\n",
    "test_augment = dict(featurewise_center = False, \n",
    "                      samplewise_center = False,\n",
    "                      rotation_range = 10, \n",
    "                      width_shift_range = 0.01, \n",
    "                      height_shift_range = 0.01, \n",
    "                      brightness_range = (0.8,1.2),\n",
    "                      shear_range = 0.01,\n",
    "                      zoom_range = [1, 1.25],  \n",
    "                      horizontal_flip = True, \n",
    "                      vertical_flip = False,\n",
    "                      fill_mode = 'reflect',\n",
    "                      data_format = 'channels_last')\n",
    "aug_test_generator = augment(test_generator, test_augment) \n",
    "aug_test_iterator = iter(aug_test_generator)\n",
    "\n",
    "X_batch, y_batch = next(aug_test_iterator)\n",
    "X, y = (X_batch[0], y_batch[0])\n",
    "plt.imshow(X.astype(int))\n",
    "plt.show()\n",
    "plt.title('person mask from ground truth')\n",
    "plt.imshow(y[:,:,catMapper.get('person', 'category_name', 'filter_idx')])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_generator = augment(cocoDataGenerator(val_annotations, 'datasets/coco2017/images/val2017', catMapper, batch_size=8, shuffle=True))\n",
    "\n",
    "# train_generator = cocoDataGenerator(train_annotations, 'datasets/coco2017/images/train2017', catMapper, batch_size=8, shuffle=True)\n",
    "# train_augment = dict(featurewise_center = False, \n",
    "#                       samplewise_center = False,\n",
    "#                       rotation_range = 10, \n",
    "#                       width_shift_range = 0.01, \n",
    "#                       height_shift_range = 0.01, \n",
    "#                       brightness_range = (0.8,1.2),\n",
    "#                       shear_range = 0.01,\n",
    "#                       zoom_range = [1, 1.25],  \n",
    "#                       horizontal_flip = True, \n",
    "#                       vertical_flip = False,\n",
    "#                       fill_mode = 'reflect',\n",
    "#                       data_format = 'channels_last')\n",
    "# train_aug_generator = augment(train_generator, train_augment)                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for a a few epics to sanity check\n",
    "\n",
    "# from architectures import u_net_x5\n",
    "# import importlib\n",
    "# importlib.reload(u_net_x5)\n",
    "\n",
    "# model = u_net_x5.define_unet()\n",
    "# model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), # 5 started working without label smoothing\n",
    "#               loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n",
    "#               metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt-get -y update\n",
    "# !apt-get -y install graphviz\n",
    "# model.summary()\n",
    "# tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_history = model.fit(train_aug_generator, epochs=5,\n",
    "#                           steps_per_epoch=100,\n",
    "#                           validation_steps=20,\n",
    "#                           validation_data=val_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_argmax_masks(img_path, model, input_size=(128,128), binary=False):\n",
    "  img = np.asarray(Image.open(img_path))\n",
    "  original_img_dims = (img.shape[0], img.shape[1])\n",
    "  X = np.array([cv2.resize(img, input_size)])\n",
    "  y = model.predict(X)\n",
    "  y = np.squeeze(y)\n",
    "  # y = np.squeeze(y)[:,:,:-1]\n",
    "\n",
    "  \n",
    "\n",
    "  if binary:\n",
    "    # out = y[:,:,1]\n",
    "    # out = y[:,:,1] > 0.3\n",
    "    out = tf.argmax(y, axis=-1)\n",
    "\n",
    "    plt.imshow(np.squeeze(X))\n",
    "    plt.show()\n",
    "    plt.imshow(out)\n",
    "    plt.show()\n",
    "  else:\n",
    "    # print(y)\n",
    "    out = np.apply_along_axis(np.argmax, 2, y)\n",
    "\n",
    "    cat_pixel_counts = np.bincount(out.flatten())\n",
    "    above_zero = np.count_nonzero(cat_pixel_counts > 0)\n",
    "    top_n = above_zero if above_zero < 5 else 5\n",
    "    most_freq_idxs = np.argpartition(cat_pixel_counts, -top_n)[-top_n:]\n",
    "    print(most_freq_idxs)\n",
    "  \n",
    "    plt.imshow(np.squeeze(X))\n",
    "    plt.show()\n",
    "\n",
    "    masks = np.zeros(input_size)\n",
    "    for i, filter_idx in enumerate(most_freq_idxs[:-1]):\n",
    "        pixel_value = i + 1\n",
    "        masks = np.maximum((out == filter_idx)*pixel_value, masks)\n",
    "    plt.imshow(masks)\n",
    "    plt.show()\n",
    "\n",
    "    for i in most_freq_idxs[::-1]:\n",
    "      plt.title(catMapper.get(i, 'filter_idx', 'category_name'))\n",
    "      plt.imshow(out == i)\n",
    "      plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full Results from script along with tensorboard and results from best model on validation images and never before seen images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load well trained model and run show functions on it\n",
    "\n",
    "dir = '../tf/notebooks/portfolio/image_segmentation'\n",
    "\n",
    "final_model = tf.keras.models.load_model(f'{dir}/models/test_model7.h5', compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_argmax_masks('datasets/coco2017/images/val2017/000000150265.jpg', final_model, input_size=(224,224))\n",
    "show_argmax_masks('datasets/coco2017/images/val2017/000000148707.jpg', final_model, input_size=(224,224))\n",
    "show_argmax_masks('datasets/coco2017/images/val2017/000000290843.jpg', final_model, input_size=(224,224))\n",
    "show_argmax_masks('datasets/coco2017/images/val2017/000000291861.jpg', final_model, input_size=(224,224))\n",
    "show_argmax_masks('datasets/coco2017/images/val2017/000000291551.jpg', final_model, input_size=(224,224))\n",
    "show_argmax_masks('datasets/coco2017/images/val2017/000000149406.jpg', final_model, input_size=(224,224))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_argmax_masks(f'{dir}/assets/dog.jpg', final_model, input_size=(224,224))\n",
    "show_argmax_masks(f'{dir}/assets/cat.jpg', final_model, input_size=(224,224))\n",
    "show_argmax_masks(f'{dir}/assets/frisbee.jpg', final_model, input_size=(224,224))\n",
    "show_argmax_masks(f'{dir}/assets/hot_dog.jpg', final_model, input_size=(224,224))\n",
    "show_argmax_masks(f'{dir}/assets/tennis.jpg', final_model, input_size=(224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls datasets/coco2017/images/val2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(Image.open('datasets/coco2017/images/val2017/000000150265.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(Image.open('datasets/coco2017/images/val2017/000000438304.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(Image.open('datasets/coco2017/images/val2017/000000148707.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
